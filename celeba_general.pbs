#!/bin/bash
#PBS -l select=1:ncpus=8:mem=256gb:ngpus=1
#PBS -l walltime=00:30:00
#PBS -N full_celeba_uni
#PBS -o /rds/general/user/eso18/home/mmvmvae/logs_mmvmvae/run_celeba_uni.log
#PBS -e /rds/general/user/eso18/home/mmvmvae/logs_mmvmvae/run_celeba_uni.err

eval "$(~/anaconda3/bin/conda shell.bash hook)"
source activate mvvae
cd mmvmvae

# wandb login $WANDB_API_KEY
export CUDA_LAUNCH_BLOCKING=1
export TORCH_USE_CUDA_DSA=1
export HYDRA_FULL_ERROR=1

device="cuda"  # 'cuda' if you are useing a GPU
models=("mixedprior") # "joint" or "mixedprior" or "drpm" "mixedpriorstdnorm"
dataset_names=("CelebA")
seeds=(1)
betas=(0.25 0.5 1.0 2.0 4.0)
betas=(1.0)
gammas=(0.0001)
latent_dims=(128)
drpm_prior=(False)
alpha_annealing=(True)
alpha_annealing_n_steps=(150000)
n_epochs=(400)
learning_rates=(5e-4)
batch_sizes=(256)
log_freq_downstream=1
log_freq_coherence=1
log_freq_lhood=500
log_freq_plotting=1

# run the classifier first
# python main_train_clf_celeba.py

# run the classifier with a jointposterior
python main_mv_wsl.py  dataset="CelebA" model="mixedprior" \
      ++model.learning_rate=5e-6 \

