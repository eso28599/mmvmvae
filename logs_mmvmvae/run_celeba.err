wandb: Currently logged in as: eso18 (eso18-imperial-college-london) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Seed set to 0
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
wandb: Tracking run with wandb version 0.19.10
wandb: Run data is saved locally in /rds/general/user/eso18/home/mmvmvae/clfs/wandb/run-20250428_144403-ajzy68du
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-planet-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/eso18-imperial-college-london/multi_view_vae
wandb: üöÄ View run at https://wandb.ai/eso18-imperial-college-london/multi_view_vae/runs/ajzy68du
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e70f6c73-8e66-30b0-7946-3d7e596671fd]

  | Name       | Type       | Params
------------------------------------------
0 | encoders   | ModuleList | 9.0 M 
1 | decoders   | ModuleList | 9.0 M 
2 | transforms | Compose    | 0     
------------------------------------------
18.0 M    Trainable params
0         Non-trainable params
18.0 M    Total params
72.138    Total estimated model params size (MB)
Error executing job with overrides: ['dataset=CelebA', 'model=joint']
Traceback (most recent call last):
  File "/rds/general/user/eso18/home/mmvmvae/main_mv_wsl.py", line 147, in <module>
    run_experiment()
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/rds/general/user/eso18/home/mmvmvae/main_mv_wsl.py", line 98, in run_experiment
    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
    self._run_sanity_check()
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1062, in _run_sanity_check
    val_loop.run()
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 134, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 391, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/rds/general/user/eso18/home/anaconda3/envs/mvvae/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 403, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/rds/general/user/eso18/home/mmvmvae/mv_vaes/mv_vae.py", line 157, in validation_step
    out = self.forward(batch)
TypeError: MVJointVAE.forward() missing 1 required positional argument: 'resample'
